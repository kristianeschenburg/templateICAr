m2b <- lm(acpt ~ slim + shld + trks, data=Highway2)
y_adj <- residuals(m2a)
x_adj <- residuals(m2b)
qplot(x=x_adj, y=y_adj) +
geom_smooth() + theme_few() +
ggtitle('Added Variable Plot, Adjusted for Speed Limit, Shoulder Width & Truck Vol') +
xlab('Access Points (Adjusted)') +
ylab('Accident Rate (Adjusted)')
#Model 3: AccidentRate ~ Speed Limit + Shoulder Width + Truck Volume + Access Points
lm3 <- lm(rate ~ slim + shld + trks + acpt, data=Highway2)
summary(lm3)
library(ggplot2)
library(ggthemes)
#install.packages('alr4')
library(alr4)
#Response Var: Accident Rate
#Potential Predictor Vars: Pct Truck Volume, Num Access points, Speed Limit, Shoulder Width
Highway2 <- Highway[,c('rate','trks','acpt','slim','shld')]
scatterplotMatrix(Highway2)
#Model 1: AccidentRate ~ Speed Limit + Shoulder Width
lm1 <- lm(rate ~ slim + shld, data=Highway2)
summary(lm1)
qplot(x=x_adj, y=y_adj) +
geom_smooth() + theme_few() +
ggtitle('Added Variable Plot, Adjusted for Speed Limit and Shoulder Width') +
xlab('Truck Volume (Adjusted)') +
ylab('Accident Rate (Adjusted)')
#Added variable plot
m1a <- lm(rate ~ slim + shld, data=Highway2)
m1b <- lm(trks ~ slim + shld, data=Highway2)
y_adj <- residuals(m1a)
x_adj <- residuals(m1b)
qplot(x=x_adj, y=y_adj) +
geom_smooth() + theme_few() +
ggtitle('Added Variable Plot, Adjusted for Speed Limit and Shoulder Width') +
xlab('Truck Volume (Adjusted)') +
ylab('Accident Rate (Adjusted)')
ggplot(data=Highway2, aes(x=trks, y=rate)) +
geom_smooth() + theme_few() +
ggtitle('Accident Rate vs Truck Volume') +
xlab('Truck Volume') +
ylab('Accident Rate')
ggplot(data=Highway2, aes(x=trks, y=rate)) + geom_point() +
geom_smooth() + theme_few() +
ggtitle('Accident Rate vs Truck Volume') +
xlab('Truck Volume') +
ylab('Accident Rate')
#Model 2: AccidentRate ~ Speed Limit + Shoulder Width + Truck Volume
lm2 <- lm(rate ~ slim + shld + trks, data=Highway2)
summary(lm2)
m2a <- lm(rate ~ slim + shld + trks, data=Highway2)
m2b <- lm(acpt ~ slim + shld + trks, data=Highway2)
y_adj <- residuals(m2a)
x_adj <- residuals(m2b)
qplot(x=x_adj, y=y_adj) +
geom_smooth() + theme_few() +
ggtitle('Added Variable Plot, Adjusted for Speed Limit, Shoulder Width & Truck Vol') +
xlab('Access Points (Adjusted)') +
ylab('Accident Rate (Adjusted)')
ggplot(data=Highway2, aes(x=acpt, y=rate)) + geom_point() +
geom_smooth() + theme_few() +
ggtitle('Accident Rate vs Access Points') +
xlab('Access Points') +
ylab('Accident Rate')
#Model 3: AccidentRate ~ Speed Limit + Shoulder Width + Truck Volume + Access Points
lm3 <- lm(rate ~ slim + shld + trks + acpt, data=Highway2)
summary(lm3)
#Model 4: AccidentRate ~ Truck Volume + Access Points
lm4 <- lm(rate ~ trks + acpt, data=Highway2)
summary(lm4)
library(roxygen2)
roxygenize()
roxygenize()
roxygenize()
roxygenize()
roxygenize()
roxygenize()
library(templateICAr)
roxygenize()
library(templateICAr)
roxygenize()
library(templateICAr)
library(templateICAr)
roxygenize()
library(usethis)
use_build_ignore('R/ignore.R')
roxygenize()
library(templateICAr)
roxygenize()
library(templateICAr)
roxygenize()
library(templateICAr)
library(devtools)
install_github('mandymejia/ciftiTools', ref='1.5')
roxygenize()
library(roxygen2)
roxygenize()
library(templateICAr)
roxygenize()
library(templateICAr)
library(oro.nifti)
help(writeNIfTI)
setwd('/Volumes/GoogleDrive/My Drive/ALS-BayesianGLM/DiagnosticICAAnalysis/MelodicICA/DiagnosticICA_20200728/FOLD_1')
GICA_fname <- 'melodic_IC.nii.gz'
mask_fname <- 'mask.nii.gz'
nifti_fnames2=NULL
scale=TRUE
verbose=TRUE
out_fname = 'test_out'
# Check arguments.
if (!is.logical(scale) || length(scale) != 1) { stop('scale must be a logical value') }
# Read GICA result
if(verbose) cat('\n Reading in GICA result')
GICA <- readNIfTI(GICA_fname, reorient = FALSE)
mask <- readNIfTI(mask_fname, reorient = FALSE)
vals <- sort(unique(mask))
vals
length(vals)
dim(mask)
vals <- sort(unique(as.vector(mask)))
vals
dims <- dim(mask)
if(any(!(vals %in% c(0,1)))) stop('Mask must be binary.')
V <- sum(mask)
V
Q <- dim(GICA)[4]
Q
GICA_mat <- matrix(NA, nvox, Q)
GICA_mat <- matrix(NA, V, Q)
for(q in 1:Q){
GICA_mat[,q] <- GICA[,,,q][mask==1]
}
head(GICA_mat)
# Center each IC map.
GICA_mat <- scale(GICA_mat, scale=FALSE)
if(verbose){
cat(paste0('\n Number of data locations: ',V))
cat(paste0('\n Number of group ICs: ',Q))
}
L <- Q
inds=NULL
L <- Q
if(!is.null(inds)){
if(any(!(inds %in% 1:Q))) stop('Invalid entries in inds argument.')
L <- length(inds)
} else {
inds <- 1:Q
}
inds
if(use_nifti) N <- length(cifti_fnames)
N <- length(cifti_fnames)
N <- length(nifti_fnames)
test_data_dir <- '/Volumes/GoogleDrive/My Drive/ALS-BayesianGLM/DiagnosticICAAnalysis/MelodicICA/DiagnosticICA_20200728/TEST_1/als'
train_data_dir <- '/Volumes/GoogleDrive/My Drive/ALS-BayesianGLM/DiagnosticICAAnalysis/MelodicICA/DiagnosticICA_20200728/TEST_1/als'
train_subj <- list.files(train_data_dir)
train_subj
train_subj <- list.dirs(train_data_dir)
train_subj
help(list.dirs)
train_subj <- list.dirs(train_data_dir, full.names=FALSE)
train_subj
train_subj <- train_subj[train_subj != '']
train_subj
getwd()
nifti_fnames <- file.path(train_data_dir, train_subj, 'preprocessedData_240.nii')
N <- length(nifti_fnames)
if(verbose){
cat(paste0('\n Number of template ICs: ',L))
cat(paste0('\n Number of training subjects: ',N))
}
nifti_fnames
if(!is.null(nifti_fnames2)) retest <- TRUE
retest
nifti_fnames2=NULL
if(!is.null(nifti_fnames2)) retest <- TRUE else retest <- FALSE
retest
if(retest){
if(length(nifti_fnames) != length(nifti_fnames2)) stop('If provided, nifti_fnames2 must have same length as nifti_fnames and be in the same subject order.')
}
# PERFORM DUAL REGRESSION ON (PSEUDO) TEST-RETEST DATA
DR1 <- DR2 <- array(NA, dim=c(N, L, V))
missing_data <- NULL
ntime_vec <- c()
ii=1
if(verbose) cat(paste0('\n Reading in data for subject ',ii,' of ',N))
#read in BOLD
fname_ii <- nifti_fnames[ii]
fname_ii
if(!file.exists(fname_ii)) {
missing_data <- c(missing_data, fname_ii)
if(verbose) cat(paste0('\n Data not available'))
next
}
BOLD1_ii <- readNIfTI(fname_ii, reorient = TRUE)
ntime <- dim(BOLD1_ii)[4]
ntime
ntime_vec <- c(ntime_vec, ntime)
if(length(ntime_vec)>2){
if(var(ntime_vec) > 0) stop('All BOLD timeseries should have the same duration')
}
BOLD1_ii_mat <- matrix(NA, nvox, ntime)
BOLD1_ii_mat <- matrix(NA, V, ntime)
for(t in 1:ntime){
BOLD1_ii_mat[,t] <- BOLD1_ii[,,,t][mask==1]
}
if(nrow(BOLD1_ii_mat) != nrow(GICA_mat)) stop(paste0('The number of data locations in GICA and BOLD timeseries data from subject ',ii,' do not match.'))
rm(BOLD1_ii)
#read in BOLD retest data OR create pseudo test-retest data
if(!retest){
part1 <- 1:round(ntime/2)
part2 <- setdiff(1:ntime, part1)
BOLD2_ii_mat <- BOLD1_ii_mat[,part2]
BOLD1_ii_mat <- BOLD1_ii_mat[,part1]
} else {
#read in BOLD from retest
fname_ii <- nifti_fnames2[ii]
if(!file.exists(fname_ii)) {
missing_data <- c(missing_data, fname_ii)
if(verbose) cat(paste0('\n Data not available'))
next
}
BOLD2_ii <- readNIfTI(fname_ii, reorient = TRUE)
if(dim(BOLD2_ii)[4] != ntime) stop('Retest BOLD data has different duration from first session.')
BOLD2_ii_mat <- matrix(NA, nvox, ntime)
for(t in 1:ntime){
BOLD2_ii_mat[,t] <- BOLD2_ii[,,,t][mask==1]
}
rm(BOLD2_ii)
if(nrow(BOLD2_ii_mat) != nrow(GICA_mat)) stop(paste0('The number of data locations in GICA and BOLD retest timeseries data from subject ',ii,' do not match.'))
}
dim(BOLD1_ii_mat)
tmp <- rowVars(BOLD1_ii_mat)
library(matrixStats)
tmp <- rowVars(BOLD1_ii_mat)
min(tmp)
sum(tmp==0)
tmp2 <- rowVars(BOLD2_ii_mat)
min(tmp2)
sum(tmp2==0)
flat_vox <- ((rowVars(BOLD1_ii_mat)==0) | (rowVars(BOLD2_ii_mat)==0))
sum(flat_vox)
length(flat_vox)
mask2 = mask
mask2[mask2==1] <- (!flat_vox)
sum(mask)
sum(mask2)
dim(GICA_mat)
dim(BOLD1_ii_mat)
if(sum(flat_vox)>0) {
warning(paste0(sum(flat_vox), ' flat voxels detected. Removing these from the mask for this and future subjects. Updated mask will be returned with estimated templates.'))
mask2[mask2==1] <- (!flat_vox)
GICA_mat <- GICA_mat[!flat_vox,]
BOLD1_ii_mat <- BOLD1_ii_mat[!flat_vox,]
BOLD2_ii_mat <- BOLD2_ii_mat[!flat_vox,]
}
mask2 = mask
warning(paste0(sum(flat_vox), ' flat voxels detected. Removing these from the mask for this and future subjects. Updated mask will be returned with estimated templates.'))
mask2[mask2==1] <- (!flat_vox)
GICA_mat <- GICA_mat[!flat_vox,]
dim(GICA_mat)
head(flat_vox)
nvox
V
BOLD1_ii_mat <- BOLD1_ii_mat[!flat_vox,]
dim(BOLD1_ii_mat)
dim(BOLD2_ii_mat)
V
V <- sum(mask2)
#perform dual regression on test and retest data
DR1_ii <- dual_reg(BOLD1_ii_mat, GICA_mat, scale=scale)$S
#' Dual Regression
#'
#' @param dat Subject-level fMRI data (VxT)
#' @param GICA Group-level independent components (VxQ)
#' @param scale A logical value indicating whether the fMRI timeseries should be scaled by the image standard deviation.
#'
#' @return A list containing the subject-level independent components S (VxQ), subject-level mixing matrix A (TxQ), and the row- and column- centered fMRI data (VxT)
#' @export
#' @importFrom matrixStats colVars
#'
dual_reg <- function(dat, GICA, scale=FALSE){
ntime <- ncol(dat) #length of timeseries
nvox <- nrow(dat) #number of data locations
if(ntime > nvox) warning('More time points than voxels. Are you sure?')
if(nvox != nrow(GICA)) stop('The number of voxels in dat and GICA must match')
Q <- ncol(GICA) #number of ICs
if(Q > nvox) warning('More ICs than voxels. Are you sure?')
if(Q > ntime) warning('More ICs than time points. Are you sure?')
#center timeseries data across space and time (and standardize scale if scale=TRUE)
dat_ctr <- scale_BOLD(dat, scale=scale)
dat_ctr_t <- t(dat_ctr)
#center each group IC over voxels
GICA <- scale(GICA, scale=FALSE)
#estimate A (IC timeseries)
A <- dat_ctr_t %*% GICA %*% solve(t(GICA) %*% GICA)
#fix scale of timeseries (sd=1)
sd_A <- sqrt(colVars(A))
D <- diag(1/sd_A)
A <- A %*% D
#estimate S (IC maps)
S <- solve(a=(t(A) %*% A), b=(t(A) %*% dat_ctr_t))
#return result
result <- list(S = S, A = A, dat_ctr = dat_ctr)
return(result)
}
#perform dual regression on test and retest data
DR1_ii <- dual_reg(BOLD1_ii_mat, GICA_mat, scale=scale)$S
#' and in the second step treats voxels as observations. Neither regression includes an intercept, so the BOLD timeseries must be centered across
#' both space and time. The group independent components used in the first regression must also be centered across space.  The mixing matrix estimated
#' in the first step of dual regression is also centered across time as a result.
#'
#' In order to ensure that all fMRI data share the same scale, the BOLD data can also be scaled by the global image standard deviation, equal to
#' \deqn{\sqrt{\frac{1}{T}\sum_{t=1}^T \sigma^2_t}},
#' where \eqn{\sigma^2_t} is the standard deviation across all voxels at time point \eqn{t}. If scaling is applied to the BOLD timeseries used in
#' template estimation, it should also be applied to the BOLD timeseries being analyzed with template ICA using the resulting templates to ensure
#' compatibility of scale.
#'
scale_BOLD <- function(BOLD, scale=FALSE){
dat <- BOLD
ntime <- ncol(dat) #length of timeseries
nvox <- nrow(dat) #number of data locations
if(ntime > nvox) warning('More time points than voxels. Are you sure?')
#center timeseries data across space and time and standardize scale
dat_t <- scale(t(dat), scale=FALSE) #center each voxel time series (remove mean image)
sig <- sqrt(mean(rowVars(dat))) #variance across image, averaged across time, square root to get SD
dat <- scale(t(dat_t), scale=FALSE) #center each image (centering across space)
if(scale) dat <- dat/sig #standardize by global SD
dat_ctr <- dat
return(dat_ctr)
}
#perform dual regression on test and retest data
DR1_ii <- dual_reg(BOLD1_ii_mat, GICA_mat, scale=scale)$S
DR2_ii <- dual_reg(BOLD2_ii_mat, GICA_mat, scale=scale)$S
DR1[ii,,] <- DR1_ii[inds,]
dim(DR1)
DR1 <- DR1[,,!flat_vox]
DR2 <- DR2[,,!flat_vox]
DR1[ii,,] <- DR1_ii[inds,]
DR2[ii,,] <- DR2_ii[inds,]
dim(DR1_ii)
for(ii in 2:N){
### READ IN BOLD DATA AND PERFORM DUAL REGRESSION
if(verbose) cat(paste0('\n Reading in data for subject ',ii,' of ',N))
#read in BOLD
fname_ii <- nifti_fnames[ii]
if(!file.exists(fname_ii)) {
missing_data <- c(missing_data, fname_ii)
if(verbose) cat(paste0('\n Data not available'))
next
}
BOLD1_ii <- readNIfTI(fname_ii, reorient = TRUE)
ntime <- dim(BOLD1_ii)[4]
ntime_vec <- c(ntime_vec, ntime)
if(length(ntime_vec)>2){
if(var(ntime_vec) > 0) stop('All BOLD timeseries should have the same duration')
}
BOLD1_ii_mat <- matrix(NA, V, ntime)
for(t in 1:ntime){
BOLD1_ii_mat[,t] <- BOLD1_ii[,,,t][mask2==1]
}
if(nrow(BOLD1_ii_mat) != nrow(GICA_mat)) stop(paste0('The number of data locations in GICA and BOLD timeseries data from subject ',ii,' do not match.'))
rm(BOLD1_ii)
#read in BOLD retest data OR create pseudo test-retest data
if(!retest){
part1 <- 1:round(ntime/2)
part2 <- setdiff(1:ntime, part1)
BOLD2_ii_mat <- BOLD1_ii_mat[,part2]
BOLD1_ii_mat <- BOLD1_ii_mat[,part1]
} else {
#read in BOLD from retest
fname_ii <- nifti_fnames2[ii]
if(!file.exists(fname_ii)) {
missing_data <- c(missing_data, fname_ii)
if(verbose) cat(paste0('\n Data not available'))
next
}
BOLD2_ii <- readNIfTI(fname_ii, reorient = TRUE)
if(dim(BOLD2_ii)[4] != ntime) stop('Retest BOLD data has different duration from first session.')
BOLD2_ii_mat <- matrix(NA, nvox, ntime)
for(t in 1:ntime){
BOLD2_ii_mat[,t] <- BOLD2_ii[,,,t][mask2==1]
}
rm(BOLD2_ii)
if(nrow(BOLD2_ii_mat) != nrow(GICA_mat)) stop(paste0('The number of data locations in GICA and BOLD retest timeseries data from subject ',ii,' do not match.'))
}
flat_vox <- ((rowVars(BOLD1_ii_mat)==0) | (rowVars(BOLD2_ii_mat)==0))
if(sum(flat_vox)>0) {
warning(paste0(sum(flat_vox), ' flat voxels detected. Removing these from the mask for this and future subjects. Updated mask will be returned with estimated templates.'))
mask2[mask2==1] <- (!flat_vox)
GICA_mat <- GICA_mat[!flat_vox,]
BOLD1_ii_mat <- BOLD1_ii_mat[!flat_vox,]
BOLD2_ii_mat <- BOLD2_ii_mat[!flat_vox,]
DR1 <- DR1[,,!flat_vox]
DR2 <- DR2[,,!flat_vox]
V <- sum(mask2)
}
#perform dual regression on test and retest data
DR1_ii <- dual_reg(BOLD1_ii_mat, GICA_mat, scale=scale)$S
DR2_ii <- dual_reg(BOLD2_ii_mat, GICA_mat, scale=scale)$S
DR1[ii,,] <- DR1_ii[inds,]
DR2[ii,,] <- DR2_ii[inds,]
}
if(verbose) cat('\n Estimating Template Mean')
V
cat(paste0('Total number of voxels in updated mask: ', V, '\n'))
mean1 <- apply(DR1, c(2,3), mean, na.rm=TRUE)
mean2 <- apply(DR2, c(2,3), mean, na.rm=TRUE)
template_mean <- t((mean1 + mean2)/2)
sum(is.na(template_mean))
class(template_mean)
# total variance
if(verbose) cat('\n Estimating Total Variance')
var_tot1 <- apply(DR1, c(2,3), var, na.rm=TRUE)
var_tot2 <- apply(DR2, c(2,3), var, na.rm=TRUE)
var_tot <- t((var_tot1 + var_tot2)/2)
# noise (within-subject) variance
if(verbose) cat('\n Estimating Within-Subject Variance')
DR_diff = DR1 - DR2;
var_noise <- t((1/2)*apply(DR_diff, c(2,3), var, na.rm=TRUE))
# signal (between-subject) variance
if(verbose) cat('\n Estimating Template (Between-Subject) Variance')
template_var <- var_tot - var_noise
template_var[template_var < 0] <- 0
sum(is.na(template_var))
out_fname_mean <- paste0(out_fname, '_mean')
out_fname_mean
writeNIfTI(template_mean, out_fname_mean)
writeNIfTI(template_var, out_fname_var)
out_fname_var <- paste0(out_fname, '_var')
writeNIfTI(template_var, out_fname_var)
getwd()
dim(template_mean)
template_mean_nifti <- array(NA, dim = dims)
BOLD1_ii <- readNIfTI(fname_ii, reorient = TRUE)
if(any(dim(BOLD1_ii)[-4] != dims)) stop('BOLD dims and mask dims do not match')
dim(BOLD1_ii)[-4]
dims
template_mean_nifti <- template_var_nifti <- array(NA, dim = c(dims, L))
dim(template_mean)
for(l in 1:L){
template_mean_nifti[,,,l] <- template_mean[,l]
template_var_nifti[,,,l] <- template_var[,l]
}
dim(template_mean[,l])
length(template_mean[,l])
V
template_mean_nifti[,,,l][mask2==1] <- template_mean[,l]
for(l in 1:L){
template_mean_nifti[,,,l][mask2==1] <- template_mean[,l]
template_var_nifti[,,,l][mask2==1] <- template_var[,l]
}
writeNIfTI(template_mean, out_fname_mean)
writeNIfTI(template_var, out_fname_var)
dim(mask2)
writeNIfTI(mask2, 'mask2')
summary(template_mean)
writeNIfTI(template_mean_nifti, out_fname_mean)
writeNIfTI(template_var_nifti, out_fname_var)
dim(template_mean_nifti[,,,l])
dim(mask2)
img_test <- mask2
img_test[mask2==1] <- template_mean[,1]
writeNIfTI(img_test, "test")
dim(template_mean)
img_tmp[mask2==1] <- template_mean[,l]
template_mean_nifti <- template_var_nifti <- array(NA, dim = c(, L))
img_tmp <- mask2
for(l in 1:L){
img_tmp[mask2==1] <- template_mean[,l]
template_mean_nifti[,,,l] <- img_tmp
img_tmp[mask2==1] <- template_var[,l]
template_var_nifti[,,,l] <- img_tmp
}
template_mean_nifti <- template_var_nifti <- array(NA, dim = c(dims, L))
img_tmp <- mask2
for(l in 1:L){
img_tmp[mask2==1] <- template_mean[,l]
template_mean_nifti[,,,l] <- img_tmp
img_tmp[mask2==1] <- template_var[,l]
template_var_nifti[,,,l] <- img_tmp
}
writeNIfTI(template_mean_nifti, out_fname_mean)
img_tmp <- mask2; img_tmp[mask2==0] <- NA
for(l in 1:L){
img_tmp[mask2==1] <- template_mean[,l]
template_mean_nifti[,,,l] <- img_tmp
img_tmp[mask2==1] <- template_var[,l]
template_var_nifti[,,,l] <- img_tmp
}
writeNIfTI(template_mean_nifti, out_fname_mean)
table(mask2)
img_tmp <- mask2; #img_tmp[mask2==0] <- NA
for(l in 1:L){
img_tmp[mask2==1] <- template_mean[,l]
template_mean_nifti[,,,l] <- img_tmp
img_tmp[mask2==1] <- template_var[,l]
template_var_nifti[,,,l] <- img_tmp
}
writeNIfTI(template_mean_nifti, out_fname_mean)
template_mean_nifti <- template_var_nifti <- array(NA, dim = c(dims, L))
img_tmp <- mask2
for(l in 1:L){
img_tmp[mask2==1] <- template_mean[,l]
template_mean_nifti[,,,l] <- img_tmp
img_tmp[mask2==1] <- template_var[,l]
template_var_nifti[,,,l] <- img_tmp
}
writeNIfTI(template_mean_nifti, out_fname_mean)
writeNIfTI(template_var_nifti, out_fname_var)
writeNIfTI(mask2, 'mask2')
summary(template_var)
library(ciftiTools)
